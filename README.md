# ğŸ¤– Fine-Tuned GPT-2 Text Generator

This repository contains a Colab notebook and training files for fine-tuning OpenAI's GPT-2 model on a custom dataset using the Hugging Face `transformers` library.

---
## ğŸ’¾ Model Access

You can download the fine-tuned GPT-2 model here:

ğŸ“ [Google Drive â€“ GPT-2 Fine-Tuned Model](https://drive.google.com/drive/folders/1pjfA3AvVYr-_laz0rSU_ZVnM8D8PDViS?usp=sharing)



## ğŸ“Œ Task
**Task 1 - Prodigy Internship**  
Train a language model to generate coherent and contextually relevant text based on a given prompt using GPT-2.

---

## ğŸ§  Model
The model used is [GPT-2](https://huggingface.co/gpt2), a transformer-based language model developed by OpenAI.

I fine-tuned GPT-2 on a custom text dataset that includes motivational, poetic, and AI-related content. After training, the model is able to generate new paragraphs that follow the style of the training data.

---

## ğŸ“‚ Files in this Repo
| File | Description |
|------|-------------|
| `task1 prodigy ai.ipynb` | Main Colab notebook for fine-tuning |
| `custom_data.txt` | Custom dataset used to train GPT-2 |
| `generated_sample.txt` | Sample output generated after fine-tuning |
| `README.md` | This file |

---

## ğŸ“ Sample Prompt & Output
![Screenshot 2025-07-06 143248](https://github.com/user-attachments/assets/5e6ef94b-fc56-4820-99ec-4fd45b5aa4d7)
**Prompt:**
```text
The future of technology is

